(.venv) C:\Users\Acer\Desktop\GITHUB_DEAN\face-vit-phq>python train.py
Using device: cuda
Number of GPUs available: 1
Current GPU: NVIDIA GeForce RTX 3060 Laptop GPU
Loading dataset from deanngkl/ferplus-7cls (split=train) ...
Loaded dataset from deanngkl/ferplus-7cls (split=train), length=35481
Loaded 35481 samples from deanngkl/ferplus-7cls
Loading dataset from deanngkl/ferplus-7cls (split=validation) ...
No explicit validation split for deanngkl/ferplus-7cls, doing manual split.
Loading dataset from deanngkl/affectnet_no_contempt (split=train) ...
Loaded dataset from deanngkl/affectnet_no_contempt (split=train), length=27823
Loaded 27823 samples from deanngkl/affectnet_no_contempt
Loading dataset from deanngkl/affectnet_no_contempt (split=validation) ...
No explicit validation split for deanngkl/affectnet_no_contempt, doing manual split.
Loading dataset from deanngkl/raf-db-7emotions (split=train) ...
Loaded dataset from deanngkl/raf-db-7emotions (split=train), length=20471
Loaded 20471 samples from deanngkl/raf-db-7emotions
Loading dataset from deanngkl/raf-db-7emotions (split=validation) ...
No explicit validation split for deanngkl/raf-db-7emotions, doing manual split.
⮕  Using WeightedRandomSampler:
   class 1 → 3394 samples → weight 0.000
   class 2 → 4330 samples → weight 0.000
   class 3 → 18396 samples → weight 0.000
   class 4 → 20877 samples → weight 0.000
   class 5 → 9270 samples → weight 0.000
   class 6 → 9383 samples → weight 0.000
Loaded 75398 training samples from 3 sources
Loaded 8377 validation samples from 3 sources
Training-set distribution:
  0: 0 : 9748
  1: 1 : 3394
  2: 2 : 4330
  3: 3 : 18396
  4: 4 : 20877
  5: 5 : 9270
  6: 6 : 9383
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k)  
INFO:timm.models._hub:[timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Using optimizer: AdamW with learning rate 0.0003
2025-06-12 12:10:44.233583: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-12 12:10:48.682866: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
DataLoader sanity-check …
First batch loaded: torch.Size([64, 3, 224, 224]), targets: torch.Size([64]), Unique targets: tensor([0, 1, 2, 3, 4, 5, 6])
DataLoader test passed. Starting training loop...
Epoch 1 started at 2025-06-12 12:12:09
63.)
  x = F.scaled_dot_product_attention(
c:\Users\Acer\Desktop\GITHUB_DEAN\face-vit-phq\.venv\Lib\site-packages\torch\optim\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
c:\Users\Acer\Desktop\GITHUB_DEAN\face-vit-phq\.venv\Lib\site-packages\torch\optim\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1 finished at 2025-06-12 12:16:34
Epoch [1/60], Train Loss: 1.2244, Train Accuracy: 0.5399, Time: 00:04:24
LR end-of-epoch: 3.00e-04
Epoch [1/60]  train_loss=1.2244  val_loss=0.8037  val_acc=0.7049  LR=3.00e-04
Epoch 2 started at 2025-06-12 12:18:19
Epoch 2 finished at 2025-06-12 12:23:26
Epoch [2/60], Train Loss: 1.0076, Train Accuracy: 0.6272, Time: 00:05:07
LR end-of-epoch: 2.99e-04
Epoch [2/60]  train_loss=1.0076  val_loss=0.7866  val_acc=0.6955  LR=2.99e-04
Best model saved at output\best_model_epoch_2.pth
Epoch 3 started at 2025-06-12 12:24:47
Epoch 3 finished at 2025-06-12 12:29:57
Epoch [3/60], Train Loss: 0.9454, Train Accuracy: 0.6500, Time: 00:05:10
LR end-of-epoch: 2.98e-04
Epoch [3/60]  train_loss=0.9454  val_loss=0.8355  val_acc=0.6882  LR=2.98e-04
Epoch 4 started at 2025-06-12 12:31:31
Epoch 4 finished at 2025-06-12 12:36:40
Epoch [4/60], Train Loss: 0.8772, Train Accuracy: 0.6776, Time: 00:05:09
LR end-of-epoch: 2.96e-04
Epoch [4/60]  train_loss=0.8772  val_loss=0.6722  val_acc=0.7511  LR=2.96e-04
Epoch 5 started at 2025-06-12 12:38:02
Epoch 5 finished at 2025-06-12 12:42:20
Epoch [5/60], Train Loss: 0.8518, Train Accuracy: 0.6843, Time: 00:04:17
LR end-of-epoch: 2.94e-04
Epoch [5/60]  train_loss=0.8518  val_loss=0.6022  val_acc=0.7744  LR=2.94e-04
              precision    recall  f1-score   support

           0      0.676     0.712     0.693      1085
           1      0.451     0.669     0.539       393
           2      0.548     0.687     0.610       476
           3      0.911     0.872     0.891      1971
           4      0.873     0.863     0.868      2379
           5      0.679     0.661     0.670      1047
           6      0.829     0.644     0.725      1026

    accuracy                          0.774      8377
   macro avg      0.710     0.730     0.714      8377
weighted avg      0.789     0.774     0.778      8377

Best model saved at output\best_model_epoch_5.pth
Epoch 6 started at 2025-06-12 12:43:32
Epoch 6 finished at 2025-06-12 12:47:42
Epoch [6/60], Train Loss: 0.7882, Train Accuracy: 0.7114, Time: 00:04:09
LR end-of-epoch: 2.91e-04
Epoch [6/60]  train_loss=0.7882  val_loss=0.6148  val_acc=0.7735  LR=2.91e-04
Best model saved at output\best_model_epoch_6.pth
Epoch 7 started at 2025-06-12 12:48:33
Epoch 7 finished at 2025-06-12 12:52:22
Epoch [7/60], Train Loss: 0.7938, Train Accuracy: 0.7175, Time: 00:03:49
LR end-of-epoch: 2.88e-04
Epoch [7/60]  train_loss=0.7938  val_loss=0.5977  val_acc=0.7811  LR=2.88e-04
Epoch 8 started at 2025-06-12 12:53:13
Epoch 8 finished at 2025-06-12 12:57:57
Epoch [8/60], Train Loss: 0.7356, Train Accuracy: 0.7339, Time: 00:04:43
LR end-of-epoch: 2.85e-04
Epoch [8/60]  train_loss=0.7356  val_loss=0.6072  val_acc=0.7772  LR=2.85e-04
Best model saved at output\best_model_epoch_8.pth
Epoch 9 started at 2025-06-12 12:59:21
Epoch 9 finished at 2025-06-12 13:05:39
Epoch [9/60], Train Loss: 0.7411, Train Accuracy: 0.7229, Time: 00:06:17
LR end-of-epoch: 2.81e-04
Epoch [9/60]  train_loss=0.7411  val_loss=0.6217  val_acc=0.7776  LR=2.81e-04
Epoch 10 started at 2025-06-12 13:07:17
Epoch 10 finished at 2025-06-12 13:13:35
Epoch [10/60], Train Loss: 0.7160, Train Accuracy: 0.7428, Time: 00:06:18
LR end-of-epoch: 2.76e-04
Epoch [10/60]  train_loss=0.7160  val_loss=0.5985  val_acc=0.7889  LR=2.76e-04
              precision    recall  f1-score   support

           0      0.728     0.696     0.712      1085
           1      0.555     0.641     0.595       393
           2      0.533     0.630     0.577       476
           3      0.925     0.891     0.908      1971
           4      0.896     0.855     0.875      2379
           5      0.645     0.719     0.680      1047
           6      0.767     0.738     0.752      1026

    accuracy                          0.789      8377
   macro avg      0.721     0.739     0.729      8377
weighted avg      0.797     0.789     0.792      8377

Epoch 11 started at 2025-06-12 13:15:29
Epoch 11 finished at 2025-06-12 13:21:39
Epoch [11/60], Train Loss: 0.6991, Train Accuracy: 0.7497, Time: 00:06:10
LR end-of-epoch: 2.71e-04
Epoch [11/60]  train_loss=0.6991  val_loss=0.5943  val_acc=0.7931  LR=2.71e-04
Best model saved at output\best_model_epoch_11.pth
Epoch 12 started at 2025-06-12 13:22:53
Epoch 12 finished at 2025-06-12 13:28:18
Epoch [12/60], Train Loss: 0.6615, Train Accuracy: 0.7650, Time: 00:05:24
LR end-of-epoch: 2.66e-04
Epoch [12/60]  train_loss=0.6615  val_loss=0.6567  val_acc=0.7751  LR=2.66e-04
Best model saved at output\best_model_epoch_12.pth
Epoch 13 started at 2025-06-12 13:29:55
Epoch 13 finished at 2025-06-12 13:36:13
Epoch [13/60], Train Loss: 0.6358, Train Accuracy: 0.7707, Time: 00:06:17
LR end-of-epoch: 2.61e-04
Epoch [13/60]  train_loss=0.6358  val_loss=0.6152  val_acc=0.7887  LR=2.61e-04
Epoch 14 started at 2025-06-12 13:37:50
Epoch 14 finished at 2025-06-12 13:43:22
Epoch [14/60], Train Loss: 0.6111, Train Accuracy: 0.7830, Time: 00:05:31
LR end-of-epoch: 2.55e-04
Epoch [14/60]  train_loss=0.6111  val_loss=0.6292  val_acc=0.7899  LR=2.55e-04
Epoch 15 started at 2025-06-12 13:44:37
Epoch 15 finished at 2025-06-12 13:48:57
Epoch [15/60], Train Loss: 0.5900, Train Accuracy: 0.7922, Time: 00:04:19
LR end-of-epoch: 2.48e-04
Epoch [15/60]  train_loss=0.5900  val_loss=0.6296  val_acc=0.7993  LR=2.48e-04
              precision    recall  f1-score   support

           0      0.708     0.775     0.740      1085
           1      0.574     0.573     0.573       393
           2      0.639     0.576     0.606       476
           3      0.945     0.882     0.912      1971
           4      0.880     0.873     0.876      2379
           5      0.667     0.695     0.681      1047
           6      0.756     0.791     0.773      1026

    accuracy                          0.799      8377
   macro avg      0.738     0.738     0.737      8377
weighted avg      0.803     0.799     0.800      8377

Epoch 16 started at 2025-06-12 13:50:12
Epoch 16 finished at 2025-06-12 13:55:44
Epoch [16/60], Train Loss: 0.5766, Train Accuracy: 0.7914, Time: 00:05:32
LR end-of-epoch: 2.42e-04
Epoch [16/60]  train_loss=0.5766  val_loss=0.6776  val_acc=0.7833  LR=2.42e-04
Best model saved at output\best_model_epoch_16.pth
Epoch 17 started at 2025-06-12 13:56:59
Epoch 17 finished at 2025-06-12 14:01:29
Epoch [17/60], Train Loss: 0.5511, Train Accuracy: 0.8013, Time: 00:04:29
LR end-of-epoch: 2.35e-04
Epoch [17/60]  train_loss=0.5511  val_loss=0.6705  val_acc=0.7930  LR=2.35e-04
Epoch 18 started at 2025-06-12 14:02:42
Epoch 18 finished at 2025-06-12 14:06:50
Epoch [18/60], Train Loss: 0.5528, Train Accuracy: 0.7974, Time: 00:04:08
LR end-of-epoch: 2.28e-04
Epoch [18/60]  train_loss=0.5528  val_loss=0.6676  val_acc=0.7953  LR=2.28e-04
Epoch 19 started at 2025-06-12 14:08:00
Epoch 19 finished at 2025-06-12 14:13:05
Epoch [19/60], Train Loss: 0.5363, Train Accuracy: 0.8023, Time: 00:05:05
LR end-of-epoch: 2.20e-04
Epoch [19/60]  train_loss=0.5363  val_loss=0.6471  val_acc=0.7952  LR=2.20e-04
Epoch 20 started at 2025-06-12 14:14:36
Epoch 20 finished at 2025-06-12 14:21:20
Epoch [20/60], Train Loss: 0.5022, Train Accuracy: 0.8178, Time: 00:06:44
LR end-of-epoch: 2.13e-04
Epoch [20/60]  train_loss=0.5022  val_loss=0.7133  val_acc=0.7910  LR=2.13e-04
              precision    recall  f1-score   support

           0      0.717     0.717     0.717      1085
           1      0.586     0.529     0.556       393
           2      0.560     0.582     0.571       476
           3      0.923     0.906     0.914      1971
           4      0.892     0.854     0.873      2379
           5      0.652     0.729     0.688      1047
           6      0.738     0.764     0.751      1026

    accuracy                          0.791      8377
   macro avg      0.724     0.726     0.724      8377
weighted avg      0.795     0.791     0.792      8377

Epoch 21 started at 2025-06-12 14:22:58
Epoch 21 finished at 2025-06-12 14:29:54
Epoch [21/60], Train Loss: 0.4969, Train Accuracy: 0.8198, Time: 00:06:56
LR end-of-epoch: 2.05e-04
Epoch [21/60]  train_loss=0.4969  val_loss=0.6959  val_acc=0.7996  LR=2.05e-04
Epoch 22 started at 2025-06-12 14:31:21
Epoch 22 finished at 2025-06-12 14:39:23
Epoch [22/60], Train Loss: 0.4649, Train Accuracy: 0.8316, Time: 00:08:02
LR end-of-epoch: 1.97e-04
Epoch [22/60]  train_loss=0.4649  val_loss=0.7698  val_acc=0.7899  LR=1.97e-04
Best model saved at output\best_model_epoch_22.pth
Epoch 23 started at 2025-06-12 14:41:46
Epoch 23 finished at 2025-06-12 14:49:34
Epoch [23/60], Train Loss: 0.4794, Train Accuracy: 0.8266, Time: 00:07:47
LR end-of-epoch: 1.89e-04
Epoch [23/60]  train_loss=0.4794  val_loss=0.7310  val_acc=0.8026  LR=1.89e-04
Epoch 24 started at 2025-06-12 14:51:18
Epoch 24 finished at 2025-06-12 14:57:10
Epoch [24/60], Train Loss: 0.4426, Train Accuracy: 0.8371, Time: 00:05:51
LR end-of-epoch: 1.80e-04
Epoch [24/60]  train_loss=0.4426  val_loss=0.7650  val_acc=0.7978  LR=1.80e-04
Best model saved at output\best_model_epoch_24.pth
Epoch 25 started at 2025-06-12 14:58:26
Epoch 25 finished at 2025-06-12 15:04:21
Epoch [25/60], Train Loss: 0.4395, Train Accuracy: 0.8391, Time: 00:05:54
LR end-of-epoch: 1.72e-04
Epoch [25/60]  train_loss=0.4395  val_loss=0.7370  val_acc=0.8029  LR=1.72e-04
              precision    recall  f1-score   support

           0      0.739     0.743     0.741      1085
           1      0.565     0.606     0.585       393
           2      0.650     0.542     0.591       476
           3      0.893     0.938     0.915      1971
           4      0.917     0.839     0.876      2379
           5      0.677     0.726     0.701      1047
           6      0.746     0.800     0.772      1026

    accuracy                          0.803      8377
   macro avg      0.741     0.742     0.740      8377
weighted avg      0.806     0.803     0.803      8377

Epoch 26 started at 2025-06-12 15:06:03
Epoch 26 finished at 2025-06-12 15:11:44
Epoch [26/60], Train Loss: 0.4464, Train Accuracy: 0.8388, Time: 00:05:40
LR end-of-epoch: 1.63e-04
Epoch [26/60]  train_loss=0.4464  val_loss=0.8317  val_acc=0.8002  LR=1.63e-04
Best model saved at output\best_model_epoch_26.pth
Epoch 27 started at 2025-06-12 15:12:56
Epoch 27 finished at 2025-06-12 15:17:44
Epoch [27/60], Train Loss: 0.4216, Train Accuracy: 0.8420, Time: 00:04:48
LR end-of-epoch: 1.55e-04
Epoch [27/60]  train_loss=0.4216  val_loss=0.8181  val_acc=0.8046  LR=1.55e-04
Epoch 28 started at 2025-06-12 15:19:03
Epoch 28 finished at 2025-06-12 15:24:15
Epoch [28/60], Train Loss: 0.4221, Train Accuracy: 0.8488, Time: 00:05:12
LR end-of-epoch: 1.46e-04
Epoch [28/60]  train_loss=0.4221  val_loss=0.8094  val_acc=0.7986  LR=1.46e-04
Best model saved at output\best_model_epoch_28.pth
Epoch 29 started at 2025-06-12 15:26:18
Epoch 29 finished at 2025-06-12 15:32:08
Epoch [29/60], Train Loss: 0.4113, Train Accuracy: 0.8409, Time: 00:05:49
LR end-of-epoch: 1.38e-04
Epoch [29/60]  train_loss=0.4113  val_loss=0.8262  val_acc=0.8041  LR=1.38e-04
Epoch 30 started at 2025-06-12 15:33:19
Epoch 30 finished at 2025-06-12 15:38:36
Epoch [30/60], Train Loss: 0.4025, Train Accuracy: 0.8551, Time: 00:05:16
LR end-of-epoch: 1.29e-04
Epoch [30/60]  train_loss=0.4025  val_loss=0.8445  val_acc=0.8028  LR=1.29e-04
              precision    recall  f1-score   support

           0      0.750     0.686     0.716      1085
           1      0.601     0.606     0.603       393
           2      0.619     0.523     0.567       476
           3      0.931     0.918     0.925      1971
           4      0.904     0.879     0.891      2379
           5      0.615     0.777     0.687      1047
           6      0.773     0.761     0.767      1026

    accuracy                          0.803      8377
   macro avg      0.742     0.736     0.737      8377
weighted avg      0.808     0.803     0.804      8377

Epoch 31 started at 2025-06-12 15:40:17
Epoch 31 finished at 2025-06-12 15:46:06
Epoch [31/60], Train Loss: 0.3702, Train Accuracy: 0.8633, Time: 00:05:49
LR end-of-epoch: 1.21e-04
Epoch [31/60]  train_loss=0.3702  val_loss=0.8950  val_acc=0.7995  LR=1.21e-04
Epoch 32 started at 2025-06-12 15:47:18
Epoch 32 finished at 2025-06-12 15:52:50
Epoch [32/60], Train Loss: 0.4027, Train Accuracy: 0.8576, Time: 00:05:31
LR end-of-epoch: 1.13e-04
Epoch [32/60]  train_loss=0.4027  val_loss=0.8482  val_acc=0.8072  LR=1.13e-04
Epoch 33 started at 2025-06-12 15:55:00
Epoch 33 finished at 2025-06-12 16:01:38
Epoch [33/60], Train Loss: 0.3868, Train Accuracy: 0.8497, Time: 00:06:37
LR end-of-epoch: 1.04e-04
Epoch [33/60]  train_loss=0.3868  val_loss=0.9077  val_acc=0.8086  LR=1.04e-04
Best model saved at output\best_model_epoch_33.pth
Epoch 34 started at 2025-06-12 16:02:50
Epoch 34 finished at 2025-06-12 16:08:19
Epoch [34/60], Train Loss: 0.3540, Train Accuracy: 0.8694, Time: 00:05:29
LR end-of-epoch: 9.63e-05
Epoch [34/60]  train_loss=0.3540  val_loss=0.9202  val_acc=0.8061  LR=9.63e-05
Best model saved at output\best_model_epoch_34.pth
Epoch 35 started at 2025-06-12 16:09:31
Epoch 35 finished at 2025-06-12 16:15:47
Epoch [35/60], Train Loss: 0.3675, Train Accuracy: 0.8705, Time: 00:06:16
LR end-of-epoch: 8.84e-05
Epoch [35/60]  train_loss=0.3675  val_loss=0.8929  val_acc=0.8143  LR=8.84e-05
              precision    recall  f1-score   support

           0      0.749     0.747     0.748      1085
           1      0.597     0.588     0.592       393
           2      0.621     0.559     0.588       476
           3      0.921     0.938     0.930      1971
           4      0.892     0.900     0.896      2379
           5      0.707     0.676     0.691      1047
           6      0.761     0.794     0.777      1026

    accuracy                          0.814      8377
   macro avg      0.750     0.743     0.746      8377
weighted avg      0.812     0.814     0.813      8377

Epoch 36 started at 2025-06-12 16:16:57
Epoch 36 finished at 2025-06-12 16:22:41
Epoch [36/60], Train Loss: 0.3646, Train Accuracy: 0.8680, Time: 00:05:43
LR end-of-epoch: 8.08e-05
Epoch [36/60]  train_loss=0.3646  val_loss=0.9313  val_acc=0.8131  LR=8.08e-05
Best model saved at output\best_model_epoch_36.pth
Epoch 37 started at 2025-06-12 16:23:53
Epoch 37 finished at 2025-06-12 16:33:06
Epoch [37/60], Train Loss: 0.3760, Train Accuracy: 0.8549, Time: 00:09:13
LR end-of-epoch: 7.33e-05
Epoch [37/60]  train_loss=0.3760  val_loss=0.9567  val_acc=0.8079  LR=7.33e-05
Epoch 38 started at 2025-06-12 16:34:49
Epoch 38 finished at 2025-06-12 16:42:21
Epoch [38/60], Train Loss: 0.3627, Train Accuracy: 0.8539, Time: 00:07:32
LR end-of-epoch: 6.61e-05
Epoch [38/60]  train_loss=0.3627  val_loss=0.9364  val_acc=0.8160  LR=6.61e-05
Epoch 39 started at 2025-06-12 16:43:57
Epoch 39 finished at 2025-06-12 16:51:06
Epoch [39/60], Train Loss: 0.3152, Train Accuracy: 0.8834, Time: 00:07:08
LR end-of-epoch: 5.92e-05
Epoch [39/60]  train_loss=0.3152  val_loss=0.9714  val_acc=0.8152  LR=5.92e-05
Best model saved at output\best_model_epoch_39.pth
Epoch 40 started at 2025-06-12 16:52:44
Epoch 40 finished at 2025-06-12 16:58:40
Epoch [40/60], Train Loss: 0.3380, Train Accuracy: 0.8708, Time: 00:05:56
LR end-of-epoch: 5.26e-05
Epoch [40/60]  train_loss=0.3380  val_loss=1.0047  val_acc=0.8126  LR=5.26e-05
              precision    recall  f1-score   support

           0      0.708     0.783     0.744      1085
           1      0.663     0.560     0.607       393
           2      0.572     0.534     0.552       476
           3      0.940     0.923     0.931      1971
           4      0.895     0.908     0.902      2379
           5      0.715     0.672     0.693      1047
           6      0.750     0.778     0.764      1026

    accuracy                          0.813      8377
   macro avg      0.749     0.737     0.742      8377
weighted avg      0.812     0.813     0.812      8377

Epoch 41 started at 2025-06-12 17:01:12
Epoch 41 finished at 2025-06-12 17:08:40
Epoch [41/60], Train Loss: 0.3280, Train Accuracy: 0.8679, Time: 00:07:28
LR end-of-epoch: 4.63e-05
Epoch [41/60]  train_loss=0.3280  val_loss=1.0124  val_acc=0.8162  LR=4.63e-05
Epoch 42 started at 2025-06-12 17:10:48
Epoch 42 finished at 2025-06-12 17:20:49
Epoch [42/60], Train Loss: 0.3526, Train Accuracy: 0.8675, Time: 00:10:01
LR end-of-epoch: 4.04e-05
Epoch [42/60]  train_loss=0.3526  val_loss=1.0530  val_acc=0.8090  LR=4.04e-05
Best model saved at output\best_model_epoch_42.pth
Epoch 43 started at 2025-06-12 17:22:29
Epoch 43 finished at 2025-06-12 17:30:10
Epoch [43/60], Train Loss: 0.3348, Train Accuracy: 0.8645, Time: 00:07:41
LR end-of-epoch: 3.48e-05
Epoch [43/60]  train_loss=0.3348  val_loss=1.0440  val_acc=0.8138  LR=3.48e-05
Epoch 44 started at 2025-06-12 17:31:50
Epoch 44 finished at 2025-06-12 17:37:20
Epoch [44/60], Train Loss: 0.3168, Train Accuracy: 0.8714, Time: 00:05:30
LR end-of-epoch: 2.96e-05
Epoch [44/60]  train_loss=0.3168  val_loss=1.0478  val_acc=0.8119  LR=2.96e-05
Epoch 45 started at 2025-06-12 17:39:30
Epoch 45 finished at 2025-06-12 17:48:20
Epoch [45/60], Train Loss: 0.3168, Train Accuracy: 0.8790, Time: 00:08:49
LR end-of-epoch: 2.48e-05
Epoch [45/60]  train_loss=0.3168  val_loss=1.0615  val_acc=0.8165  LR=2.48e-05
              precision    recall  f1-score   support

           0      0.727     0.766     0.746      1085
           1      0.669     0.504     0.575       393
           2      0.612     0.500     0.550       476
           3      0.936     0.933     0.934      1971
           4      0.889     0.915     0.902      2379
           5      0.708     0.709     0.708      1047
           6      0.750     0.795     0.772      1026
0.741      8377
weighted avg      0.813     0.817     0.814      8377

Epoch 46 started at 2025-06-12 17:51:12
Epoch 46 finished at 2025-06-12 17:59:18
Epoch [46/60], Train Loss: 0.3142, Train Accuracy: 0.8706, Time: 00:08:06
LR end-of-epoch: 2.03e-05
Epoch [46/60]  train_loss=0.3142  val_loss=1.0768  val_acc=0.8165  LR=2.03e-05
Best model saved at output\best_model_epoch_46.pth
Epoch 47 started at 2025-06-12 18:02:09
Epoch 47 finished at 2025-06-12 18:09:18
Epoch [47/60], Train Loss: 0.3077, Train Accuracy: 0.8789, Time: 00:07:09
LR end-of-epoch: 1.64e-05
Epoch [47/60]  train_loss=0.3077  val_loss=1.0976  val_acc=0.8157  LR=1.64e-05
Epoch 48 started at 2025-06-12 18:10:45
Epoch 48 finished at 2025-06-12 18:20:04
Epoch [48/60], Train Loss: 0.3243, Train Accuracy: 0.8699, Time: 00:09:19
LR end-of-epoch: 1.28e-05
Epoch [48/60]  train_loss=0.3243  val_loss=1.0900  val_acc=0.8163  LR=1.28e-05
Epoch 49 started at 2025-06-12 18:22:00
Epoch 49 finished at 2025-06-12 18:27:27
Epoch [49/60], Train Loss: 0.2877, Train Accuracy: 0.8926, Time: 00:05:27
LR end-of-epoch: 9.71e-06
Epoch [49/60]  train_loss=0.2877  val_loss=1.1075  val_acc=0.8205  LR=9.71e-06
Epoch 50 started at 2025-06-12 18:28:48
Epoch 50 finished at 2025-06-12 18:33:02
Epoch [50/60], Train Loss: 0.3253, Train Accuracy: 0.8736, Time: 00:04:13
LR end-of-epoch: 7.07e-06
Epoch [50/60]  train_loss=0.3253  val_loss=1.0745  val_acc=0.8217  LR=7.07e-06
              precision    recall  f1-score   support

           0      0.733     0.759     0.746      1085
           1      0.685     0.532     0.599       393
           2      0.616     0.542     0.577       476
           3      0.934     0.937     0.936      1971
           4      0.895     0.918     0.906      2379
           5      0.704     0.713     0.709      1047
           6      0.775     0.794     0.784      1026

    accuracy                          0.822      8377
   macro avg      0.763     0.742     0.751      8377
weighted avg      0.819     0.822     0.820      8377

Best model saved at output\best_model_epoch_50.pth
Epoch 51 started at 2025-06-12 18:34:13
Epoch 51 finished at 2025-06-12 18:38:19
Epoch [51/60], Train Loss: 0.3151, Train Accuracy: 0.8773, Time: 00:04:06
LR end-of-epoch: 4.89e-06
Epoch [51/60]  train_loss=0.3151  val_loss=1.1270  val_acc=0.8141  LR=4.89e-06
Best model saved at output\best_model_epoch_51.pth
Epoch 52 started at 2025-06-12 18:39:29
Epoch 52 finished at 2025-06-12 18:43:37
Epoch [52/60], Train Loss: 0.3209, Train Accuracy: 0.8694, Time: 00:04:08
LR end-of-epoch: 3.20e-06
Epoch [52/60]  train_loss=0.3209  val_loss=1.1061  val_acc=0.8199  LR=3.20e-06
Epoch 53 started at 2025-06-12 18:44:47
Epoch 53 finished at 2025-06-12 18:48:55
Epoch [53/60], Train Loss: 0.3028, Train Accuracy: 0.8750, Time: 00:04:08
LR end-of-epoch: 1.98e-06
Epoch [53/60]  train_loss=0.3028  val_loss=1.1139  val_acc=0.8162  LR=1.98e-06
Epoch 54 started at 2025-06-12 18:50:08
Epoch 54 finished at 2025-06-12 18:54:21
Epoch [54/60], Train Loss: 0.3147, Train Accuracy: 0.8700, Time: 00:04:12
LR end-of-epoch: 1.25e-06
Epoch [54/60]  train_loss=0.3147  val_loss=1.1253  val_acc=0.8175  LR=1.25e-06
Epoch 55 started at 2025-06-12 18:55:14
Epoch 55 finished at 2025-06-12 18:59:23
Epoch [55/60], Train Loss: 0.3161, Train Accuracy: 0.8705, Time: 00:04:08
LR end-of-epoch: 1.00e-06
Epoch [55/60]  train_loss=0.3161  val_loss=1.0926  val_acc=0.8209  LR=1.00e-06
              precision    recall  f1-score   support

           0      0.719     0.776     0.746      1085
           1      0.675     0.491     0.568       393
           2      0.623     0.511     0.561       476
           3      0.940     0.939     0.939      1971
           4      0.891     0.924     0.907      2379
           5      0.714     0.706     0.710      1047
           6      0.767     0.791     0.779      1026

    accuracy                          0.821      8377
   macro avg      0.761     0.734     0.744      8377
weighted avg      0.817     0.821     0.818      8377

Epoch 56 started at 2025-06-12 19:00:28
Epoch 56 finished at 2025-06-12 19:04:35
Epoch [56/60], Train Loss: 0.2965, Train Accuracy: 0.8826, Time: 00:04:06
LR end-of-epoch: 1.24e-06
Epoch [56/60]  train_loss=0.2965  val_loss=1.1041  val_acc=0.8201  LR=1.24e-06
Epoch 57 started at 2025-06-12 19:05:46
Epoch 57 finished at 2025-06-12 19:09:52
Epoch [57/60], Train Loss: 0.2852, Train Accuracy: 0.8765, Time: 00:04:05
LR end-of-epoch: 1.97e-06
Epoch [57/60]  train_loss=0.2852  val_loss=1.1248  val_acc=0.8177  LR=1.97e-06
Epoch 58 started at 2025-06-12 19:10:59
Epoch 58 finished at 2025-06-12 19:15:05
Epoch [58/60], Train Loss: 0.3037, Train Accuracy: 0.8774, Time: 00:04:06
LR end-of-epoch: 3.18e-06
Epoch [58/60]  train_loss=0.3037  val_loss=1.1169  val_acc=0.8196  LR=3.18e-06
Epoch 59 started at 2025-06-12 19:16:15
Epoch 59 finished at 2025-06-12 19:20:27
Epoch [59/60], Train Loss: 0.3023, Train Accuracy: 0.8773, Time: 00:04:12
LR end-of-epoch: 4.88e-06
Epoch [59/60]  train_loss=0.3023  val_loss=1.1246  val_acc=0.8190  LR=4.88e-06
Epoch 60 started at 2025-06-12 19:21:38
Epoch 60 finished at 2025-06-12 19:25:44
Epoch [60/60], Train Loss: 0.3050, Train Accuracy: 0.8740, Time: 00:04:06
LR end-of-epoch: 7.05e-06
Epoch [60/60]  train_loss=0.3050  val_loss=1.1161  val_acc=0.8191  LR=7.05e-06
              precision    recall  f1-score   support

           0      0.742     0.767     0.754      1085
           1      0.681     0.489     0.569       393
           2      0.607     0.523     0.562       476
           3      0.932     0.941     0.936      1971
           4      0.887     0.922     0.904      2379
           5      0.709     0.712     0.710      1047
           6      0.759     0.777     0.768      1026

    accuracy                          0.819      8377
   macro avg      0.759     0.733     0.743      8377
weighted avg      0.815     0.819     0.816      8377

Final model saved at output\final_model.pth
Training complete.